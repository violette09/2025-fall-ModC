{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Linear Regression - Integrated Capstone Project\n",
    "\n",
    "## Overview\n",
    "This notebook covers fundamental concepts of linear regression applied to your capstone project dataset:\n",
    "- Simple and Multiple Linear Regression\n",
    "- Polynomial Terms\n",
    "- Interaction Terms\n",
    "- Multicollinearity Detection\n",
    "- Variance Inflation Factor (VIF)\n",
    "- Working with Categorical and Continuous Features\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Build and interpret linear regression models\n",
    "2. Create polynomial and interaction terms\n",
    "3. Detect and handle multicollinearity\n",
    "4. Calculate and interpret VIF\n",
    "5. Work with mixed data types (categorical and continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Your Dataset\n",
    "\n",
    "**Instructions:** Replace the sample data below with your own capstone project dataset.\n",
    "\n",
    "```python\n",
    "# Example: Load your data\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset for demonstration purposes\n",
    "# Replace this with your actual capstone project data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Generate sample data\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.randn(n_samples) * 10 + 50,\n",
    "    'feature2': np.random.randn(n_samples) * 5 + 30,\n",
    "    'feature3': np.random.randn(n_samples) * 8 + 40,\n",
    "    'category': np.random.choice(['A', 'B', 'C'], n_samples),\n",
    "    'binary_feature': np.random.choice([0, 1], n_samples)\n",
    "})\n",
    "\n",
    "# Create target variable with some relationship to features\n",
    "df['target'] = (2 * df['feature1'] + \n",
    "                1.5 * df['feature2'] + \n",
    "                0.5 * df['feature3'] + \n",
    "                np.random.randn(n_samples) * 10)\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Linear Regression\n",
    "\n",
    "We'll start with a simple linear regression using one predictor variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression: feature1 vs target\n",
    "X_simple = df[['feature1']]\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "simple_model = LinearRegression()\n",
    "simple_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = simple_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "print(\"Simple Linear Regression Results:\")\n",
    "print(f\"Coefficient: {simple_model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {simple_model.intercept_:.4f}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, alpha=0.5, label='Actual')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Simple Linear Regression: Feature 1 vs Target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiple Linear Regression\n",
    "\n",
    "Now let's use multiple features to predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression\n",
    "X_multiple = df[['feature1', 'feature2', 'feature3']]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multiple, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "multiple_model = LinearRegression()\n",
    "multiple_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = multiple_model.predict(X_test)\n",
    "\n",
    "# Display coefficients\n",
    "print(\"Multiple Linear Regression Results:\")\n",
    "print(\"\\nCoefficients:\")\n",
    "for feature, coef in zip(X_multiple.columns, multiple_model.coef_):\n",
    "    print(f\"  {feature}: {coef:.4f}\")\n",
    "print(f\"\\nIntercept: {multiple_model.intercept_:.4f}\")\n",
    "print(f\"\\nR² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actual vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Multiple Linear Regression: Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Polynomial Terms\n",
    "\n",
    "Polynomial features allow us to capture non-linear relationships using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(df[['feature1']])\n",
    "\n",
    "# Create DataFrame with polynomial features\n",
    "poly_feature_names = poly.get_feature_names_out(['feature1'])\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=poly_feature_names)\n",
    "\n",
    "print(\"Polynomial Features:\")\n",
    "print(X_poly_df.head())\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit polynomial model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = poly_model.predict(X_test)\n",
    "\n",
    "print(\"\\nPolynomial Regression Results (Degree 2):\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different polynomial degrees\n",
    "degrees = [1, 2, 3, 4]\n",
    "results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(df[['feature1']])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append({'Degree': degree, 'R²': r2, 'RMSE': rmse})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparison of Polynomial Degrees:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Terms\n",
    "\n",
    "Interaction terms capture the combined effect of two or more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction terms manually\n",
    "df_interaction = df.copy()\n",
    "df_interaction['feature1_x_feature2'] = df['feature1'] * df['feature2']\n",
    "df_interaction['feature1_x_feature3'] = df['feature1'] * df['feature3']\n",
    "df_interaction['feature2_x_feature3'] = df['feature2'] * df['feature3']\n",
    "\n",
    "print(\"Dataset with Interaction Terms:\")\n",
    "print(df_interaction[['feature1', 'feature2', 'feature3', \n",
    "                       'feature1_x_feature2', 'feature1_x_feature3', \n",
    "                       'feature2_x_feature3']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with interaction terms\n",
    "X_interaction = df_interaction[['feature1', 'feature2', 'feature3',\n",
    "                                 'feature1_x_feature2', 'feature1_x_feature3', \n",
    "                                 'feature2_x_feature3']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_interaction, y, test_size=0.2, random_state=42)\n",
    "\n",
    "interaction_model = LinearRegression()\n",
    "interaction_model.fit(X_train, y_train)\n",
    "y_pred = interaction_model.predict(X_test)\n",
    "\n",
    "print(\"Model with Interaction Terms:\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "\n",
    "print(\"\\nCoefficients:\")\n",
    "for feature, coef in zip(X_interaction.columns, interaction_model.coef_):\n",
    "    print(f\"  {feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multicollinearity Detection\n",
    "\n",
    "Multicollinearity occurs when predictor variables are highly correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df[['feature1', 'feature2', 'feature3', 'target']].corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly correlated features\n",
    "threshold = 0.8\n",
    "high_correlation_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            high_correlation_pairs.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_correlation_pairs:\n",
    "    print(f\"\\nHighly Correlated Feature Pairs (|correlation| > {threshold}):\")\n",
    "    for pair in high_correlation_pairs:\n",
    "        print(f\"  {pair['Feature 1']} <-> {pair['Feature 2']}: {pair['Correlation']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\nNo feature pairs with correlation > {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Variance Inflation Factor (VIF)\n",
    "\n",
    "VIF quantifies the severity of multicollinearity. A VIF > 10 typically indicates problematic multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for each feature\n",
    "X_vif = df[['feature1', 'feature2', 'feature3']]\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  VIF = 1: No correlation\")\n",
    "print(\"  VIF = 1-5: Moderate correlation\")\n",
    "print(\"  VIF = 5-10: High correlation\")\n",
    "print(\"  VIF > 10: Problematic multicollinearity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VIF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(vif_data['Feature'], vif_data['VIF'])\n",
    "plt.axhline(y=5, color='orange', linestyle='--', label='Moderate threshold (VIF=5)')\n",
    "plt.axhline(y=10, color='red', linestyle='--', label='High threshold (VIF=10)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('VIF')\n",
    "plt.title('Variance Inflation Factor by Feature')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Working with Categorical Features\n",
    "\n",
    "Linear regression requires numerical inputs, so we need to encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for categorical variables\n",
    "print(\"Original categorical feature:\")\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "# Create dummy variables\n",
    "df_encoded = pd.get_dummies(df, columns=['category'], drop_first=True, dtype=int)\n",
    "\n",
    "print(\"\\nDataset after One-Hot Encoding:\")\n",
    "print(df_encoded.head())\n",
    "print(f\"\\nNew shape: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with categorical and continuous features\n",
    "feature_cols = ['feature1', 'feature2', 'feature3', 'binary_feature']\n",
    "# Add dummy variables if they exist\n",
    "dummy_cols = [col for col in df_encoded.columns if col.startswith('category_')]\n",
    "feature_cols.extend(dummy_cols)\n",
    "\n",
    "X_mixed = df_encoded[feature_cols]\n",
    "y = df_encoded['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mixed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mixed_model = LinearRegression()\n",
    "mixed_model.fit(X_train, y_train)\n",
    "y_pred = mixed_model.predict(X_test)\n",
    "\n",
    "print(\"Model with Categorical and Continuous Features:\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "\n",
    "print(\"\\nCoefficients:\")\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': mixed_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features to compare coefficients\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_mixed)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaled_model = LinearRegression()\n",
    "scaled_model.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance based on standardized coefficients\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': np.abs(scaled_model.coef_)\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (based on standardized coefficients):\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Absolute Coefficient Value (Standardized)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance in Linear Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multiple, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "residuals_train = y_train - y_pred_train\n",
    "residuals_test = y_test - y_pred_test\n",
    "\n",
    "# Create diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Residuals vs Fitted\n",
    "axes[0, 0].scatter(y_pred_test, residuals_test, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Fitted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residuals vs Fitted Values')\n",
    "\n",
    "# 2. Histogram of Residuals\n",
    "axes[0, 1].hist(residuals_test, bins=30, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Residuals')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Residuals')\n",
    "\n",
    "# 3. Q-Q Plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals_test, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot')\n",
    "\n",
    "# 4. Scale-Location Plot\n",
    "standardized_residuals = np.sqrt(np.abs(residuals_test / np.std(residuals_test)))\n",
    "axes[1, 1].scatter(y_pred_test, standardized_residuals, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Fitted Values')\n",
    "axes[1, 1].set_ylabel('√|Standardized Residuals|')\n",
    "axes[1, 1].set_title('Scale-Location Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Simple Linear Regression**: Uses one predictor to model the relationship with the target\n",
    "2. **Multiple Linear Regression**: Uses multiple predictors for better predictions\n",
    "3. **Polynomial Terms**: Capture non-linear relationships in the data\n",
    "4. **Interaction Terms**: Model the combined effect of features\n",
    "5. **Multicollinearity**: Check correlation between predictors\n",
    "6. **VIF**: Quantifies multicollinearity (VIF > 10 is problematic)\n",
    "7. **Categorical Features**: Use one-hot encoding to include in regression models\n",
    "8. **Model Diagnostics**: Residual plots help validate model assumptions\n",
    "\n",
    "### For Your Capstone Project:\n",
    "\n",
    "1. Load your actual dataset\n",
    "2. Explore relationships between features and target\n",
    "3. Check for multicollinearity using correlation matrix and VIF\n",
    "4. Create meaningful polynomial and interaction terms\n",
    "5. Handle categorical variables appropriately\n",
    "6. Validate model assumptions using diagnostic plots\n",
    "7. Document your findings and interpretations\n",
    "\n",
    "### Next Week Preview:\n",
    "In Week 2, we'll explore regularization techniques (Ridge, Lasso, Elastic Net) to improve model performance and handle multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercises for Your Dataset\n",
    "\n",
    "Apply the following to your capstone project dataset:\n",
    "\n",
    "1. **Data Exploration**\n",
    "   - Load your dataset\n",
    "   - Identify continuous and categorical features\n",
    "   - Check for missing values and outliers\n",
    "\n",
    "2. **Simple Analysis**\n",
    "   - Create scatter plots of each feature vs target\n",
    "   - Identify potential non-linear relationships\n",
    "\n",
    "3. **Multicollinearity Check**\n",
    "   - Calculate correlation matrix\n",
    "   - Compute VIF for all features\n",
    "   - Decide which features to keep/remove\n",
    "\n",
    "4. **Model Building**\n",
    "   - Build a baseline multiple regression model\n",
    "   - Add polynomial terms where appropriate\n",
    "   - Add interaction terms based on domain knowledge\n",
    "\n",
    "5. **Evaluation**\n",
    "   - Compare model performance metrics\n",
    "   - Analyze residual plots\n",
    "   - Interpret coefficients in context of your problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your capstone project work\n",
    "# TODO: Replace sample data with your actual dataset\n",
    "# TODO: Apply the concepts learned above to your data\n",
    "# TODO: Document your findings and insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
