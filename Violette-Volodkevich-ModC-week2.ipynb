{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b122d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 2 — Regularized Logistic: Ridge, Lasso, Elastic Net\n",
    "# Dataset: `carclaims 12.csv` | Target: `FraudFound`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e070b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, roc_curve, precision_recall_curve\n",
    "\n",
    "df = pd.read_csv(\"carclaims 12.csv\")\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "target = \"FraudFound\"\n",
    "yraw = df[target]\n",
    "y = yraw if yraw.dtype!=object else yraw.astype(str).str.upper().map(\n",
    "    {\"Y\":1,\"YES\":1,\"1\":1,\"TRUE\":1,\"T\":1,\"N\":0,\"NO\":0,\"0\":0,\"FALSE\":0,\"F\":0}\n",
    ").astype(int)\n",
    "X = df.drop(columns=[target])\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)\n",
    "\n",
    "numeric = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")), (\"scale\", StandardScaler())])\n",
    "categorical = Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "pre = ColumnTransformer([(\"num\", numeric, num_cols), (\"cat\", categorical, cat_cols)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8caa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.940 | F1: 0.021 | ROC-AUC: 0.813 | PR-AUC: 0.180\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Build pipeline\n",
    "pipe_fast = Pipeline([\n",
    "    (\"pre\", pre),  # your ColumnTransformer\n",
    "    (\"clf\", LogisticRegressionCV(\n",
    "        Cs=[0.01, 0.1, 1, 10],     # small grid of C values\n",
    "        cv=3,                      # only 3 folds\n",
    "        penalty=\"l2\",              # ridge penalty (fast)\n",
    "        solver=\"liblinear\",        # efficient for binary classification\n",
    "        max_iter=1000,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"roc_auc\",\n",
    "        refit=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit and evaluate\n",
    "pipe_fast.fit(X_train, y_train)\n",
    "proba = pipe_fast.predict_proba(X_test)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "f1  = f1_score(y_test, pred)\n",
    "auc = roc_auc_score(y_test, proba)\n",
    "ap  = average_precision_score(y_test, proba)\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f} | F1: {f1:.3f} | ROC-AUC: {auc:.3f} | PR-AUC: {ap:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c619746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search completed successfully.\n",
      "Best parameters: {'clf__C': 1.0, 'clf__penalty': 'l1'}\n",
      "Accuracy: 0.940 | F1: 0.011 | ROC-AUC: 0.815 | PR-AUC: 0.179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "# Small grid for speed\n",
    "param = {\n",
    "    \"clf__penalty\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "cv3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(pipe, param, scoring=\"roc_auc\", cv=cv3, n_jobs=-1, refit=True)\n",
    "\n",
    "# --- Fit safely ---\n",
    "try:\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Grid search completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Grid search failed:\", e)\n",
    "\n",
    "# --- Only continue if the search finished ---\n",
    "if hasattr(gs, \"best_estimator_\"):\n",
    "    best = gs.best_estimator_\n",
    "    proba = best.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    f1  = f1_score(y_test, pred)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    ap  = average_precision_score(y_test, proba)\n",
    "\n",
    "    print(\"Best parameters:\", gs.best_params_)\n",
    "    print(f\"Accuracy: {acc:.3f} | F1: {f1:.3f} | ROC-AUC: {auc:.3f} | PR-AUC: {ap:.3f}\")\n",
    "else:\n",
    "    print(\"No best_estimator_. Check if GridSearchCV ran successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1328012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots to figs_wk2/\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"figs_wk2\", exist_ok=True)\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "plt.figure(); plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(\"Week 2 — ROC\")\n",
    "plt.savefig(\"figs_wk2/roc.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# PR\n",
    "prec, rec, _ = precision_recall_curve(y_test, proba)\n",
    "plt.figure(); plt.plot(rec,prec)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Week 2 — Precision-Recall\")\n",
    "plt.savefig(\"figs_wk2/pr.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# Confusion\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "plt.figure(); plt.imshow(cm)\n",
    "for (i,j),v in np.ndenumerate(cm): plt.text(j,i,str(v),ha='center',va='center')\n",
    "plt.title(\"Week 2 — Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"figs_wk2/cm.png\", bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "print(\"Saved plots to figs_wk2/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
