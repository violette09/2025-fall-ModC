{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfb36d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Saved: Volodkevich_Violette_MilestoneOne_Final.docx\n"
     ]
    }
   ],
   "source": [
    "# Install missing package for python-docx in the notebook environment\n",
    "%pip install --quiet python-docx\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.oxml.ns import qn\n",
    "import datetime\n",
    "\n",
    "doc = Document()\n",
    "style = doc.styles['Normal']\n",
    "style.font.name = 'Times New Roman'\n",
    "style._element.rPr.rFonts.set(qn('w:eastAsia'), 'Times New Roman')\n",
    "style.font.size = Pt(12)\n",
    "\n",
    "# Title page\n",
    "doc.add_heading('Milestone One — Auto Insurance Fraud Detection', 0).alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "p = doc.add_paragraph(\n",
    "    'Student: Violette Similien Volodkevich\\n'\n",
    "    'Course: DX799 O1 Data Science Capstone (Fall 2025)\\n'\n",
    "    'Date: ' + datetime.datetime.now().strftime(\"%B %d, %Y\")\n",
    ")\n",
    "p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "doc.add_page_break()\n",
    "\n",
    "# Problem statement\n",
    "doc.add_heading('1. Problem Statement', level=1)\n",
    "doc.add_paragraph(\n",
    "    \"Auto insurance fraud imposes substantial costs on policyholders and insurers. \"\n",
    "    \"This project develops predictive models that flag likely fraudulent claims using structured claim, policy, and incident data \"\n",
    "    \"from the carclaims 12.csv dataset. The target variable is FraudFound (1 = fraud, 0 = non-fraud). \"\n",
    "    \"The goal is to create interpretable and scalable models to reduce fraudulent payouts and improve claim accuracy.\"\n",
    ")\n",
    "\n",
    "# Data preparation\n",
    "doc.add_heading('2. Data and Preparation', level=1)\n",
    "doc.add_paragraph(\n",
    "    \"The dataset contains both numerical and categorical features such as claim details, vehicle characteristics, and policy information. \"\n",
    "    \"Missing numerical values were imputed with the median, while missing categorical values were imputed with the mode. \"\n",
    "    \"Categorical features were one-hot encoded and numerical features were standardized for models sensitive to scale. \"\n",
    "    \"An 80/20 stratified train-test split was used to preserve class balance for FraudFound.\"\n",
    ")\n",
    "\n",
    "# Modeling summaries\n",
    "doc.add_heading('3. Modeling Summaries (Weeks 1–6)', level=1)\n",
    "weeks = [\n",
    "    (\"Week 1 — Polynomial & Interaction Terms\",\n",
    "     \"Polynomial (degree=2) and interaction features modeled non-linear relationships. \"\n",
    "     \"A logistic regression model was trained on these engineered features. Multicollinearity checked via VIF.\",\n",
    "     \"[Insert ROC, PR, Confusion Matrix plots here]\"),\n",
    "    (\"Week 2 — Regularization (Ridge, Lasso, Elastic Net)\",\n",
    "     \"L1, L2, and Elastic Net regularization compared to reduce overfitting. Hyperparameters tuned via 5-fold CV, best model selected by ROC-AUC.\",\n",
    "     \"[Insert plots here]\"),\n",
    "    (\"Week 3 — Feature Selection & Dimensionality Reduction\",\n",
    "     \"Forward/backward selection identified key predictors. PCA retained ~90% variance, followed by logistic regression on components.\",\n",
    "     \"[Insert plots here]\"),\n",
    "    (\"Week 4 — Logistic Regression & Feature Scaling\",\n",
    "     \"Baseline classification using scaled numerics and encoded categoricals. Penalty and regularization tuned for optimal bias-variance tradeoff.\",\n",
    "     \"[Insert plots here]\"),\n",
    "    (\"Week 5 — Support Vector Machines (Kernels & Regularization)\",\n",
    "     \"Linear and RBF kernels compared. Regularization (C) tuned by grid search; RBF slightly outperformed linear.\",\n",
    "     \"[Insert plots here]\"),\n",
    "    (\"Week 6 — Decision Trees & Random Forests\",\n",
    "     \"Decision Trees produced interpretable rules; Random Forests reduced overfitting through ensembling. RF achieved the best overall AUC.\",\n",
    "     \"[Insert ROC, PR, Confusion Matrix, Feature Importance plots here]\")\n",
    "]\n",
    "for title, desc, placeholder in weeks:\n",
    "    doc.add_heading(title, level=2)\n",
    "    doc.add_paragraph(desc)\n",
    "    doc.add_paragraph(placeholder)\n",
    "\n",
    "# Deep dive\n",
    "doc.add_heading('4. Deep Dive — Logistic Regression and Random Forest', level=1)\n",
    "doc.add_paragraph(\n",
    "    \"The deep dive focused on Logistic Regression (Week 4) and Random Forest (Week 6). Logistic Regression offered transparency and interpretability, \"\n",
    "    \"while Random Forest captured non-linear interactions. Both tuned using 5-fold CV. Random Forest achieved highest ROC-AUC; Logistic Regression remained an interpretable baseline.\"\n",
    ")\n",
    "\n",
    "# Overfitting\n",
    "doc.add_heading('5. Overfitting and Hyperparameter Tuning', level=1)\n",
    "doc.add_paragraph(\n",
    "    \"Overfitting controlled via cross-validation, regularization, and tree pruning. Parameters (C, α, depth) tuned using grid search. \"\n",
    "    \"Final models validated on hold-out test set for generalization.\"\n",
    ")\n",
    "\n",
    "# Metrics table\n",
    "doc.add_heading('6. Evaluation Metrics', level=1)\n",
    "table = doc.add_table(rows=1, cols=5)\n",
    "hdr = table.rows[0].cells\n",
    "hdr[0].text, hdr[1].text, hdr[2].text, hdr[3].text, hdr[4].text = \"Model\", \"Accuracy\", \"F1 Score\", \"ROC-AUC\", \"PR-AUC\"\n",
    "for wk in range(1,7):\n",
    "    row = table.add_row().cells\n",
    "    row[0].text = f\"Week {wk}\"\n",
    "    row[1].text = row[2].text = row[3].text = row[4].text = \"[ ]\"\n",
    "\n",
    "# EDA findings\n",
    "doc.add_heading('7. Expected vs. Unexpected Findings & Role of EDA', level=1)\n",
    "doc.add_paragraph(\n",
    "    \"EDA revealed strong correlations between incident severity, policy details, and fraud likelihood. Expected: higher fraud risk in single-vehicle accidents without police reports. \"\n",
    "    \"Unexpected: some demographic groups showed elevated false positives.\"\n",
    ")\n",
    "\n",
    "# Conclusion\n",
    "doc.add_heading('8. Conclusion and Next Steps', level=1)\n",
    "doc.add_paragraph(\n",
    "    \"Random Forest delivered the best discrimination power, while Logistic Regression offered interpretability. \"\n",
    "    \"Next steps include addressing class imbalance, applying SHAP for explainability, and integrating vehicle image data for hybrid detection.\"\n",
    ")\n",
    "\n",
    "# References\n",
    "doc.add_heading('References', level=1)\n",
    "refs = [\n",
    "    \"Aqqad, A. (2023). Insurance_claims. Mendeley Data. https://data.mendeley.com/datasets/992mh7dk9y/2\",\n",
    "    \"Kapoor, K. (2023). Vehicle Insurance Fraud Detection. Kaggle. https://www.kaggle.com/datasets/khusheekapoor/vehicle-insurance-fraud-detection\",\n",
    "    \"Humans In The Loop. (2023). Car Parts and Car Damages. Kaggle. https://www.kaggle.com/datasets/humansintheloop/car-parts-and-car-damages\",\n",
    "    \"FBI. (2010). Insurance Fraud. FBI. https://www.fbi.gov/stats-services/publications/insurance-fraud\",\n",
    "    \"Dell, E. (2024). How virtual inspections can benefit insurance customers. Digital Insurance. https://www.dig-in.com/news/how-virtual-inspections-can-benefit-insurance-customers\"\n",
    "]\n",
    "for ref in refs:\n",
    "    doc.add_paragraph(ref, style='List Bullet')\n",
    "\n",
    "# Appendix\n",
    "doc.add_page_break()\n",
    "doc.add_heading('Appendix — Plots and Visuals', level=1)\n",
    "doc.add_paragraph(\"Insert saved plots from each week's notebook (ROC, PR, Confusion Matrix, Feature Importance).\")\n",
    "\n",
    "# Save\n",
    "output_file = \"Volodkevich_Violette_MilestoneOne_Final.docx\"\n",
    "doc.save(output_file)\n",
    "print(f\"Saved: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
